{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://MSI:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1605139170667)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6b8c244b\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../../data/ml_scala/CitiGroup2006_2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[org.apache.spark.sql.Row] = Array([2006-01-03,490.0,493.8,481.1,492.9,1537660], [2006-01-04,488.6,491.0,483.5,483.8,1871020], [2006-01-05,484.4,487.8,484.0,486.2,1143160], [2006-01-06,488.8,489.0,482.0,486.2,1370250], [2006-01-09,486.0,487.4,483.0,483.9,1680740])\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2006-01-03,490.0,493.8,481.1,492.9,1537660]\n",
      "[2006-01-04,488.6,491.0,483.5,483.8,1871020]\n",
      "[2006-01-05,484.4,487.8,484.0,486.2,1143160]\n",
      "[2006-01-06,488.8,489.0,482.0,486.2,1370250]\n",
      "[2006-01-09,486.0,487.4,483.0,483.9,1680740]\n"
     ]
    }
   ],
   "source": [
    "for(row <- df.head(5)){\n",
    "    println(row)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Date, Open, High, Low, Close, Volume)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|      Date|              Open|             High|               Low|             Close|           Volume|\n",
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|       755|               755|              755|               755|               755|              755|\n",
      "|   mean|      null| 386.0923178807949|390.6590596026489|380.80170860927143| 385.3421456953643|6308596.382781457|\n",
      "| stddev|      null|149.32301134820133|148.5151130063523|150.53136890891344|149.83310074439177| 8099892.56297633|\n",
      "|    min|2006-01-03|              54.4|             55.3|              30.5|              37.7|           632860|\n",
      "|    max|2008-12-31|             566.0|            570.0|             555.5|             564.1|        102869289|\n",
      "+-------+----------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Volume|\n",
      "+-------+\n",
      "|1537660|\n",
      "|1871020|\n",
      "|1143160|\n",
      "|1370250|\n",
      "|1680740|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Single Column\n",
    "df.select(\"Volume\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date|Close|\n",
      "+----------+-----+\n",
      "|2006-01-03|492.9|\n",
      "|2006-01-04|483.8|\n",
      "|2006-01-05|486.2|\n",
      "|2006-01-06|486.2|\n",
      "|2006-01-09|483.9|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Multiple Columns\n",
    "df.select($\"Date\",$\"Close\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+-----------------+\n",
      "|      Date| Open| High|  Low|Close| Volume|      HighPlusLow|\n",
      "+----------+-----+-----+-----+-----+-------+-----------------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|974.9000000000001|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|            974.5|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|            971.8|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|            971.0|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|            970.4|\n",
      "+----------+-----+-----+-----+-----+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- HighPlusLow: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create New Columns\n",
    "val df2 = df.withColumn(\"HighPlusLow\",df(\"High\") + df(\"Low\"))\n",
    "df2.show(5)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              HPL|\n",
      "+-----------------+\n",
      "|974.9000000000001|\n",
      "|            974.5|\n",
      "|            971.8|\n",
      "|            971.0|\n",
      "|            970.4|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Rename Column\n",
    "df2.select(df2(\"HighPlusLow\").as(\"HPL\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._  // Allows use of $ sign SCALA notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Grab all rows where column meets single condition - SCALA notation\n",
    "df.filter($\"Close\" > 480).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Grab all rows where column meets single condition - SQL notation\n",
    "df.filter(\"Close > 480\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-20|472.1|474.0|456.3|456.9|4781930|\n",
      "|2006-01-23|460.0|463.8|457.0|460.0|2025500|\n",
      "|2006-01-24|462.9|463.6|459.9|460.1|2083740|\n",
      "|2006-01-25|461.4|463.7|460.1|462.3|1591940|\n",
      "|2006-01-26|465.5|475.5|464.5|470.1|1988600|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Filter on multiple conditions - SCALA notation\n",
    "df.filter($\"Close\" < 480 && $\"High\" < 480).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-20|472.1|474.0|456.3|456.9|4781930|\n",
      "|2006-01-23|460.0|463.8|457.0|460.0|2025500|\n",
      "|2006-01-24|462.9|463.6|459.9|460.1|2083740|\n",
      "|2006-01-25|461.4|463.7|460.1|462.3|1591940|\n",
      "|2006-01-26|465.5|475.5|464.5|470.1|1988600|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Filter on multiple conditions - SQL notation\n",
    "df.filter(\"Close < 480 AND High < 480\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CH_low: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: string, Open: double ... 4 more fields]\r\n",
       "res13: Long = 397\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assign to new dataframe\n",
    "val CH_low = df.filter($\"Close\" < 480 && $\"High\" < 480)\n",
    "CH_low.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-04-27|472.0|484.4|471.5|481.5|2464800|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Filter on equality (Have to use three '=' signs)\n",
    "df.filter($\"High\"=== 484.40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   corr(High, Low)|\n",
      "+------------------+\n",
      "|0.9992999172726325|\n",
      "+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Correlation between 2 columns\n",
    "df.select(corr(\"High\",\"Low\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Other Operations and Useful Functions\n",
    "// http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|Charlie|  120|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|    Amy|  124|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|     FB|   Carl|  870|\n",
      "|     FB|  Sarah|  350|\n",
      "+-------+-------+-----+\n",
      "\n",
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      "\n",
      "+-------+-------+-------+------------------+\n",
      "|summary|Company| Person|             Sales|\n",
      "+-------+-------+-------+------------------+\n",
      "|  count|      8|      8|                 8|\n",
      "|   mean|   null|   null|           355.875|\n",
      "| stddev|   null|   null|259.29819430807567|\n",
      "|    min|     FB|    Amy|               120|\n",
      "|    max|   MSFT|Vanessa|               870|\n",
      "+-------+-------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Company: string, Person: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../../data/ml_scala/Sales.csv\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|       340|\n",
      "|     FB|       870|\n",
      "|   MSFT|       600|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|       120|\n",
      "|     FB|       350|\n",
      "|   MSFT|       124|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   GOOG|       660|\n",
      "|     FB|      1220|\n",
      "|   MSFT|       967|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Group by One Column\n",
    "df.groupBy(\"Company\").mean().show()\n",
    "df.groupBy(\"Company\").count().show()\n",
    "df.groupBy(\"Company\").max().show()\n",
    "df.groupBy(\"Company\").min().show()\n",
    "df.groupBy(\"Company\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                    8|\n",
      "+---------------------+\n",
      "\n",
      "+-------------------+\n",
      "|sum(DISTINCT Sales)|\n",
      "+-------------------+\n",
      "|               2847|\n",
      "+-------------------+\n",
      "\n",
      "+-----------------+\n",
      "|  var_samp(Sales)|\n",
      "+-----------------+\n",
      "|67235.55357142855|\n",
      "+-----------------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(Sales)|\n",
      "+------------------+\n",
      "|259.29819430807567|\n",
      "+------------------+\n",
      "\n",
      "+--------------------+\n",
      "|  collect_set(Sales)|\n",
      "+--------------------+\n",
      "|[350, 340, 870, 1...|\n",
      "+--------------------+\n",
      "\n",
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|      2847|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Other Aggregate Functions\n",
    "// http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\n",
    "df.select(countDistinct(\"Sales\")).show() //approxCountDistinct\n",
    "df.select(sumDistinct(\"Sales\")).show()\n",
    "df.select(variance(\"Sales\")).show()\n",
    "df.select(stddev(\"Sales\")).show() //avg,max,min,sum,stddev\n",
    "df.select(collect_set(\"Sales\")).show()\n",
    "df.select(sum(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|  870|\n",
      "|   MSFT|   Tina|  600|\n",
      "|     FB|  Sarah|  350|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|   GOOG|    Sam|  200|\n",
      "|   MSFT|    Amy|  124|\n",
      "|   GOOG|Charlie|  120|\n",
      "+-------+-------+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Order by with Desc order\n",
    "df.orderBy($\"Sales\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+-------+----+-----+-----------------+\n",
      "|summary|  Id| Name|            Sales|\n",
      "+-------+----+-----+-----------------+\n",
      "|  count|   4|    2|                2|\n",
      "|   mean|null| null|            400.5|\n",
      "| stddev|null| null|78.48885271170677|\n",
      "|    min|emp1|Cindy|            345.0|\n",
      "|    max|emp4| John|            456.0|\n",
      "+-------+----+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Id: string, Name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../../data/ml_scala/ContainsNull.csv\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Drop any row with na\n",
    "df.na.drop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Drop with a threshold on number of NA in a row.  Drop any row that has less than a minimum number of non-null values\n",
    "df.na.drop(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|100.0|\n",
      "|emp2| null|100.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| Nick| null|\n",
      "|emp3| Nick|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Fill in null-values\n",
    "df.na.fill(100).show()   // Note how it auto detects column based on datatype\n",
    "df.na.fill(\"Nick\").show()  // This would work on all columns with datatype specified in the fill statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+\n",
      "|  Id|    Name|Sales|\n",
      "+----+--------+-----+\n",
      "|emp1|    John| null|\n",
      "|emp2|New Name| null|\n",
      "|emp3|New Name|345.0|\n",
      "|emp4|   Cindy|456.0|\n",
      "+----+--------+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|200.0|\n",
      "|emp2| null|200.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Specify which column to handle NA values\n",
    "df.na.fill(\"New Name\",Array(\"Name\")).show()\n",
    "df.na.fill(200, Array(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----------------+\n",
      "|summary|  Id| Name|            Sales|\n",
      "+-------+----+-----+-----------------+\n",
      "|  count|   4|    2|                2|\n",
      "|   mean|null| null|            400.5|\n",
      "| stddev|null| null|78.48885271170677|\n",
      "|    min|emp1|Cindy|            345.0|\n",
      "|    max|emp4| John|            456.0|\n",
      "+-------+----+-----+-----------------+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sample imputation using mean of Sales to fill in the missings sales values.\n",
    "df.describe().show()\n",
    "df.na.fill(400.5, Array(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----+\n",
      "|  Id|        Name|Sales|\n",
      "+----+------------+-----+\n",
      "|emp1|        John|400.5|\n",
      "|emp2|Missing Name|400.5|\n",
      "|emp3|Missing Name|345.0|\n",
      "|emp4|       Cindy|456.0|\n",
      "+----+------------+-----+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Id: string, Name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Include both steps\n",
    "val df2 = df.na.fill(400.5, Array(\"Sales\"))\n",
    "df2.na.fill(\"Missing Name\", Array(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|      Date| Open| High|  Low|Close| Volume|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "|2006-01-03|490.0|493.8|481.1|492.9|1537660|\n",
      "|2006-01-04|488.6|491.0|483.5|483.8|1871020|\n",
      "|2006-01-05|484.4|487.8|484.0|486.2|1143160|\n",
      "|2006-01-06|488.8|489.0|482.0|486.2|1370250|\n",
      "|2006-01-09|486.0|487.4|483.0|483.9|1680740|\n",
      "+----------+-----+-----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"../../data/ml_scala/CitiGroup2006_2008\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Lot's of options here\n",
    "// http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$@add_months(startDate:org.apache.spark.sql.Column,numMonths:Int):org.apache.spark.sql.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.select(month(df(\"Date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "|      2006|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.select(year(df(\"Date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2007| 477.8203984063745|\n",
      "|2006| 489.2697211155379|\n",
      "|2008|190.48893280632404|\n",
      "+----+------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Date: string, Open: double ... 5 more fields]\r\n",
       "dfavg: org.apache.spark.sql.DataFrame = [Year: int, avg(Open): double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.withColumn(\"Year\", year(df(\"Date\")))\n",
    "val dfavg = df2.groupBy(\"Year\").mean()\n",
    "\n",
    "dfavg.select($\"Year\",$\"avg(Close)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
