{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('c:/Spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Java', '20000'), ('Python', '100000'), ('Scala', '3000')]\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from a list of tuples.\n",
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n",
    "dfFromData2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x000001DE32BB0448>\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Row Type to create a dataframe\n",
    "rowData = map(lambda x: Row(*x), data)\n",
    "print(rowData)\n",
    "dfFromData3 = spark.createDataFrame(rowData, columns)\n",
    "dfFromData3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from csv, json, or text file.\n",
    "df2 = spark.read.csv('C:/SparkCourse/1800.csv')\n",
    "df2 = spark.read.json('C:/SparkCourse/Marvel_BFS_RDD.json')\n",
    "df2 = spark.read.text('C:/SparkCourse/Marvel-Names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayStructureData = [\n",
    "        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "        ]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayStructureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('languages', ArrayType(StringType()), True),\n",
    "         StructField('state', StringType(), True),\n",
    "         StructField('gender', StringType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
      "|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(arrayStructureData, arrayStructureSchema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter dataframe with a column condition.\n",
    "df.filter(df.state =='OH') \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way of doing it\n",
    "df.filter(col('state')== 'OH').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "|[Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar to sql, can just pass in what you'd include in the where clause\"\n",
    "df.filter(\"gender == 'F'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter with multiple conditons. Males from Ohio\n",
    "df.filter(\"gender == 'M' and state = 'OH'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|            name|         languages|state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|[James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
      "|  [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering on an array column.  Only those will JAVA lanugage\n",
    "df.filter(array_contains(df.languages, 'Java')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+------+\n",
      "|                name|   languages|state|gender|\n",
      "+--------------------+------------+-----+------+\n",
      "| [Julia, , Williams]|[CSharp, VB]|   OH|     F|\n",
      "|[Mike, Mary, Will...|[Python, VB]|   OH|     M|\n",
      "+--------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering on a Nested Struct columns. Last name equal to Williams.\n",
    "df.filter(df.name.lastname == 'Williams').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+--------+---+---+\n",
      "|_c0|     _c1|_c2|_c3|\n",
      "+---+--------+---+---+\n",
      "|  0|    Will| 33|385|\n",
      "|  1|Jean-Luc| 26|  2|\n",
      "|  2|    Hugh| 55|221|\n",
      "|  3|  Deanna| 40|465|\n",
      "|  4|   Quark| 68| 21|\n",
      "|  5|  Weyoun| 59|318|\n",
      "|  6|  Gowron| 37|220|\n",
      "|  7|    Will| 54|307|\n",
      "|  8|  Jadzia| 38|380|\n",
      "|  9|    Hugh| 27|181|\n",
      "| 10|     Odo| 53|191|\n",
      "| 11|     Ben| 57|372|\n",
      "| 12|   Keiko| 54|253|\n",
      "| 13|Jean-Luc| 56|444|\n",
      "| 14|    Hugh| 43| 49|\n",
      "| 15|     Rom| 36| 49|\n",
      "| 16|  Weyoun| 22|323|\n",
      "| 17|     Odo| 35| 13|\n",
      "| 18|Jean-Luc| 45|455|\n",
      "| 19|  Geordi| 60|246|\n",
      "+---+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files into a dataframe - no headers, all columns passed in as a string type.\n",
    "df = spark.read.csv('C:/SparkCourse/fakefriends.csv')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      "\n",
      "+---+--------+---+---+\n",
      "|_c0|     _c1|_c2|_c3|\n",
      "+---+--------+---+---+\n",
      "|  0|    Will| 33|385|\n",
      "|  1|Jean-Luc| 26|  2|\n",
      "|  2|    Hugh| 55|221|\n",
      "|  3|  Deanna| 40|465|\n",
      "|  4|   Quark| 68| 21|\n",
      "|  5|  Weyoun| 59|318|\n",
      "|  6|  Gowron| 37|220|\n",
      "|  7|    Will| 54|307|\n",
      "|  8|  Jadzia| 38|380|\n",
      "|  9|    Hugh| 27|181|\n",
      "| 10|     Odo| 53|191|\n",
      "| 11|     Ben| 57|372|\n",
      "| 12|   Keiko| 54|253|\n",
      "| 13|Jean-Luc| 56|444|\n",
      "| 14|    Hugh| 43| 49|\n",
      "| 15|     Rom| 36| 49|\n",
      "| 16|  Weyoun| 22|323|\n",
      "| 17|     Odo| 35| 13|\n",
      "| 18|Jean-Luc| 45|455|\n",
      "| 19|  Geordi| 60|246|\n",
      "+---+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files into a dataframe - inferSchema = True will read correct data type, \n",
    "# delimter option is bonus here and is not needed. header = 'True' option is for\n",
    "# when the first row has column names.\n",
    "df = spark.read.options(inferSchema = 'True', delimter= ',') \\\n",
    ".csv('C:/SparkCourse/fakefriends.csv')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV files into a user-specificed custom schema\n",
    "schema = StructType() \\\n",
    "    .add(\"CustomerID\", IntegerType(), True) \\\n",
    "    .add('CustomerName', StringType(), True) \\\n",
    "    .add('Age',IntegerType(), True) \\\n",
    "    .add('Friends', IntegerType(), True)\n",
    "\n",
    "df_with_schema = spark.read.format('csv') \\\n",
    "    .option('header', False) \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .option('delimeter', ',') \\\n",
    "    .schema(schema) \\\n",
    "    .load('C:/SparkCourse/fakefriends.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+-------+\n",
      "|CustomerID|CustomerName|Age|Friends|\n",
      "+----------+------------+---+-------+\n",
      "|         0|        Will| 33|    385|\n",
      "|         1|    Jean-Luc| 26|      2|\n",
      "|         2|        Hugh| 55|    221|\n",
      "|         3|      Deanna| 40|    465|\n",
      "|         4|       Quark| 68|     21|\n",
      "|         5|      Weyoun| 59|    318|\n",
      "|         6|      Gowron| 37|    220|\n",
      "|         7|        Will| 54|    307|\n",
      "|         8|      Jadzia| 38|    380|\n",
      "|         9|        Hugh| 27|    181|\n",
      "|        10|         Odo| 53|    191|\n",
      "|        11|         Ben| 57|    372|\n",
      "|        12|       Keiko| 54|    253|\n",
      "|        13|    Jean-Luc| 56|    444|\n",
      "|        14|        Hugh| 43|     49|\n",
      "|        15|         Rom| 36|     49|\n",
      "|        16|      Weyoun| 22|    323|\n",
      "|        17|         Odo| 35|     13|\n",
      "|        18|    Jean-Luc| 45|    455|\n",
      "|        19|      Geordi| 60|    246|\n",
      "|        20|         Odo| 67|    220|\n",
      "|        21|       Miles| 19|    268|\n",
      "|        22|       Quark| 30|     72|\n",
      "|        23|       Keiko| 51|    271|\n",
      "|        24|      Julian| 25|      1|\n",
      "|        25|         Ben| 21|    445|\n",
      "|        26|      Julian| 22|    100|\n",
      "|        27|       Leeta| 42|    363|\n",
      "|        28|      Martok| 49|    476|\n",
      "|        29|         Nog| 48|    364|\n",
      "|        30|       Keiko| 50|    175|\n",
      "|        31|       Miles| 39|    161|\n",
      "|        32|         Nog| 26|    281|\n",
      "|        33|       Dukat| 53|    197|\n",
      "|        34|    Jean-Luc| 43|    249|\n",
      "|        35|     Beverly| 27|    305|\n",
      "|        36|      Kasidy| 32|     81|\n",
      "|        37|      Geordi| 58|     21|\n",
      "|        38|      Deanna| 64|     65|\n",
      "|        39|        Morn| 31|    192|\n",
      "|        40|         Odo| 52|    413|\n",
      "|        41|        Hugh| 67|    167|\n",
      "|        42|       Brunt| 54|     75|\n",
      "|        43|      Guinan| 58|    345|\n",
      "|        44|       Nerys| 35|    244|\n",
      "|        45|       Dukat| 52|     77|\n",
      "|        46|        Morn| 25|     96|\n",
      "|        47|       Brunt| 24|     49|\n",
      "|        48|         Nog| 20|      1|\n",
      "|        49|        Ezri| 40|    254|\n",
      "|        50|       Quark| 51|    283|\n",
      "|        51|     Lwaxana| 36|    212|\n",
      "|        52|     Beverly| 19|    269|\n",
      "|        53|      Geordi| 62|     31|\n",
      "|        54|       Brunt| 19|      5|\n",
      "|        55|       Keiko| 41|    278|\n",
      "|        56|      Gowron| 44|    194|\n",
      "|        57|         Odo| 57|    294|\n",
      "|        58|        Hugh| 59|    158|\n",
      "|        59|        Morn| 59|    284|\n",
      "|        60|      Geordi| 20|    100|\n",
      "|        61|      Kasidy| 62|    442|\n",
      "|        62|       Keiko| 69|      9|\n",
      "|        63|    Jean-Luc| 58|     54|\n",
      "|        64|        Elim| 31|     15|\n",
      "|        65|      Guinan| 52|    169|\n",
      "|        66|      Geordi| 21|    477|\n",
      "|        67|      Jadzia| 48|    135|\n",
      "|        68|      Guinan| 33|     74|\n",
      "|        69|    Jean-Luc| 30|    204|\n",
      "|        70|       Brunt| 52|    393|\n",
      "|        71|      Geordi| 45|    184|\n",
      "|        72|      Kasidy| 22|    179|\n",
      "|        73|       Brunt| 20|    384|\n",
      "|        74|       Leeta| 65|    208|\n",
      "|        75|        Morn| 40|    459|\n",
      "|        76|        Will| 62|    201|\n",
      "|        77|      Weyoun| 40|    407|\n",
      "|        78|        Data| 61|    337|\n",
      "|        79|       Leeta| 58|    348|\n",
      "|        80|       Dukat| 67|    445|\n",
      "|        81|      Jadzia| 54|    440|\n",
      "|        82|        Hugh| 57|    465|\n",
      "|        83|      Geordi| 32|    308|\n",
      "|        84|         Ben| 28|    311|\n",
      "|        85|       Quark| 66|    383|\n",
      "|        86|        Hugh| 55|    257|\n",
      "|        87|        Ezri| 31|    481|\n",
      "|        88|         Ben| 66|    188|\n",
      "|        89|        Worf| 24|    492|\n",
      "|        90|      Kasidy| 33|    471|\n",
      "|        91|         Rom| 46|     88|\n",
      "|        92|      Gowron| 54|      7|\n",
      "|        93|        Elim| 46|     63|\n",
      "|        94|        Morn| 62|    133|\n",
      "|        95|         Odo| 29|    173|\n",
      "|        96|        Ezri| 25|    233|\n",
      "|        97|       Nerys| 69|    361|\n",
      "|        98|        Will| 44|    178|\n",
      "|        99|       Keiko| 69|    491|\n",
      "+----------+------------+---+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a CSV file with pyspark\n",
    "df_with_schema.write.option('header',True) \\\n",
    "    .option('delimter',' ') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('C:/Users/14803/SparkTutorials/fakefriends')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|  Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "schema = ['name','department','state','salary','age','bonus']\n",
    "df = spark.createDataFrame(data= simpleData, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     257000|\n",
      "|   Finance|     351000|\n",
      "| Marketing|     171000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by department and salary\n",
    "df.groupBy('department').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of employees in each department\n",
    "df.groupBy('department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n",
      "+-----+-----------+\n",
      "|state|max(salary)|\n",
      "+-----+-----------+\n",
      "|   CA|      99000|\n",
      "|   NY|      91000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate min salary of each department and the max salary by state\n",
    "df.groupby('department').min('salary').show()\n",
    "df.groupby('state').max('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by on multiple columns.  Group by department and state for total compensation.\n",
    "df.groupby('department','state').sum('salary','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+----------+\n",
      "|department|Sum_Salary|       avg_salary|min_bonus|max_bonus|max_salary|\n",
      "+----------+----------+-----------------+---------+---------+----------+\n",
      "|     Sales|    257000|85666.66666666667|    10000|    23000|     90000|\n",
      "|   Finance|    351000|          87750.0|    15000|    24000|     99000|\n",
      "| Marketing|    171000|          85500.0|    18000|    21000|     91000|\n",
      "+----------+----------+-----------------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running more that one aggreate at a time.\n",
    "df.groupby('department') \\\n",
    "    .agg(sum(\"salary\").alias('Sum_Salary'), \\\n",
    "         avg(\"salary\").alias('avg_salary'), \\\n",
    "         min('bonus').alias('min_bonus'), \\\n",
    "         max('bonus').alias('max_bonus'), \\\n",
    "         max('salary').alias('max_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------+---------+----------+\n",
      "|department|Sum_Salary|avg_salary|min_bonus|max_bonus|max_salary|\n",
      "+----------+----------+----------+---------+---------+----------+\n",
      "|   Finance|    351000|   87750.0|    15000|    24000|     99000|\n",
      "| Marketing|    171000|   85500.0|    18000|    21000|     91000|\n",
      "+----------+----------+----------+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering on aggregate data\n",
    "df.groupby('department') \\\n",
    "    .agg(sum(\"salary\").alias('Sum_Salary'), \\\n",
    "         avg(\"salary\").alias('avg_salary'), \\\n",
    "         min('bonus').alias('min_bonus'), \\\n",
    "         max('bonus').alias('max_bonus'), \\\n",
    "         max('salary').alias('max_salary')) \\\n",
    "        .where(col('min_bonus') >=15000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = ['name','department','salary']\n",
    "df = spark.createDataFrame(simpleData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|approx_count_distinct(department)|\n",
      "+---------------------------------+\n",
      "|                                3|\n",
      "+---------------------------------+\n",
      "\n",
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|  James|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|  James|     Sales|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Saif|     Sales|  4100|\n",
      "+-------+----------+------+\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#approx_count_distinct returns the count of distinct in a group\n",
    "x = df.select(approx_count_distinct('department'))\n",
    "x.show()\n",
    "df.show()\n",
    "y = df.select(approx_count_distinct('department')).collect()[0][0]\n",
    "print(str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  collect_list(name)|\n",
      "+--------------------+\n",
      "|[James, Michael, ...|\n",
      "+--------------------+\n",
      "\n",
      "['James', 'Michael', 'Robert', 'Maria', 'James', 'Scott', 'Jen', 'Jeff', 'Kumar', 'Saif']\n"
     ]
    }
   ],
   "source": [
    "# Collect LIST - returns all values from an input column with duplicates\n",
    "df.select(collect_list('name')).show()\n",
    "x = df.select(collect_list('name')).collect()\n",
    "print(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  collect_list(name)|\n",
      "+--------------------+\n",
      "|[James, Michael, ...|\n",
      "+--------------------+\n",
      "\n",
      "['Robert', 'Kumar', 'Jeff', 'Maria', 'Scott', 'Michael', 'Saif', 'Jen', 'James']\n"
     ]
    }
   ],
   "source": [
    "# Collect_set - returns all values from an input column with duplicate values removed.\n",
    "df.select(collect_list('name')).show()\n",
    "x = df.select(collect_set('name')).collect()\n",
    "print(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
